{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a0c1a3-f5f6-4b7b-a230-a56c148c881e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eee3d3\">\n",
    "<h1> 4-dimensionality_reduction.ipynb </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa435a7-4a1a-41e2-9523-bd2651ffca15",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### The purpose of this notebook is to reduce the dimension of our peak table :\n",
    "\n",
    "One main challenge in metabolomics data analysis is dealing with high dimension data, e.g. for a peak table $(n,p)$, having $p > n$ (more features than samples). It could be a problem for downstream analysis.\n",
    "\n",
    "To reduce the dimensionnality, you can use :\n",
    "- feature selection: find a subset of input features\n",
    "- feature extraction: project high-dimensional space into a space of fewer dimensions\n",
    "\n",
    "_Hint : methods that can be tested (or not ?) $\\rightarrow$ Principal Component Analysis (PCA), Partial Least Squares (PLS), Canonical Correlation Analysis (CCA), Autoencoder, ..._\n",
    "\n",
    "_Autoencoder is a type of artificial neural network, part of deep learning, you can test this method at the very end of the project if you still have time and interest in that (__huge bonus if you manage to make it work, but do it only if you have time left, the main objective of this project is to find potential biomarkers__)_\n",
    "\n",
    "---\n",
    "\n",
    "Import a peak table that you previously imputed (thus has no more missing values) and treated (transformation and/or scaling and/or normalisation).\n",
    "\n",
    "Same as before, think about quantitative/qualitative/graphic ways to present the different method outputs !\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec086d0",
   "metadata": {},
   "source": [
    "### Very nice --> https://cimcb.github.io/MetabWorkflowTutorial/Tutorial1.html\n",
    "\n",
    "**Voir les quelques notes que j'ai mise sur le drive**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4ae62",
   "metadata": {},
   "source": [
    "## 1) PCA testing (not done yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cimcb_lite as cb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]) + '/bin/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import normalisation_scaling_functions as nsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef009c",
   "metadata": {},
   "source": [
    "###  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used the mean normalization and Bayesian imputation (can use any other, I chosed randomly)\n",
    "path_peakTable = '/'.join(os.getcwd().split('/')[:-1]) + '/data/peakTable/original_peak_table/peakTable_HILIC_POS.csv'\n",
    "peakTable_HILIC_POS = pd.read_csv(path_peakTable, sep=',', decimal='.', na_values='NA')\n",
    "peakTable_HILIC_POS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237cae3",
   "metadata": {},
   "source": [
    "### Tumo Type vector (0 : Non-case ; 1 : HCC_Wide ; 2 : HCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "tumo_type=peakTable_HILIC_POS[\"TypTumo\"]\n",
    "tumo_type=tumo_type.fillna(\"Non-case\")\n",
    "tumo_type_name=copy.deepcopy(tumo_type)\n",
    "tumo_type[tumo_type==\"HCC\"]=1\n",
    "tumo_type[tumo_type==\"HCC_Wide\"]=2\n",
    "tumo_type[tumo_type==\"Non-case\"]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79c4e6",
   "metadata": {},
   "source": [
    "###   Separation en incedent et noncase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=peakTable_HILIC_POS[\"Groups\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7899fac",
   "metadata": {},
   "source": [
    "### Normlised/scaled and without NA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/'.join(os.getcwd().split('/')[:-1]) + '/data/peakTable/scaled_peak_tables/'\n",
    "filename=path+\"l1_normalisation_X_KNN_samplestest.csv\"\n",
    "data_nm = pd.read_csv(filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ed392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6eb2e",
   "metadata": {},
   "source": [
    "###  Creating PCA object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58baabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pca.fit_transform(data_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "h = ax.scatter(scores[:,1],scores[:,0],c=tumo_type)#colormap[np.array((tumo_type)).astype(int)])\n",
    "plt.xlabel('PC1', fontsize=15)\n",
    "plt.ylabel('PC2', fontsize=15)\n",
    "plt.title('Quality Control PCA plot',fontsize=20)\n",
    "legend = ax.legend(*h.legend_elements(),loc=\"lower left\", title=\"Classes\")\n",
    "ax.add_artist(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd622957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "pca=pca.fit(data_nm)\n",
    "components=pca.components_.T\n",
    "fig, ax = plt.subplots()\n",
    "h = ax.scatter(components[:,0],components[:,1],)\n",
    "plt.title(\"Components\")\n",
    "plt.xlabel(\"axe1\")\n",
    "plt.ylabel(\"axe2\")\n",
    "plt.axvline(x=0,color='black')\n",
    "plt.axhline(y=0,color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757f5db",
   "metadata": {},
   "source": [
    "###  Non case HCC Wid and HCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_nm.values                                  \n",
    "Xscale = cb.utils.scale(X, method='auto')    \n",
    "\n",
    "cb.plot.pca(Xscale,\n",
    "            pcx=1,                                                  # pc for x-axis\n",
    "            pcy=2,                                                  # pc for y-axis\n",
    "            group_label=tumo_type_name,                    # labels for Hover in PCA loadings plot\n",
    "            plot_ci=True, \n",
    "            grid_line=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453e1ef",
   "metadata": {},
   "source": [
    "## Test on all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c292939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pca_graph(scores,components,tumo_type,name,name_comp):\n",
    "    fig, ax = plt.subplots()\n",
    "    h = ax.scatter(scores[:,1],scores[:,0],c=tumo_type)#colormap[np.array((tumo_type)).astype(int)])\n",
    "    plt.xlabel('PC1', fontsize=15)\n",
    "    plt.ylabel('PC2', fontsize=15)\n",
    "    plt.title('Quality Control PCA plot',fontsize=20)\n",
    "    legend = ax.legend(*h.legend_elements(),loc=\"lower left\", title=\"Classes\")\n",
    "    ax.add_artist(legend)\n",
    "    plt.savefig(name)\n",
    "    plt.close('all')\n",
    "    fig, ax = plt.subplots()\n",
    "    h = ax.scatter(components[:,0],components[:,1],)\n",
    "    plt.savefig(name_comp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "path='/'.join(os.getcwd().split('/')[:-1]) + '/data/peakTable/scaled_peak_tables/'\n",
    "all_files = glob.glob(path + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5bc504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for filename in all_files:\n",
    "        name=filename.replace('csv', 'png')\n",
    "        name=name.replace('peakTable/scaled_peak_tables', 'graphs')\n",
    "        name_comp=name.replace('.png', '_C.png')\n",
    "        df = pd.read_csv(filename, index_col=0, header=0)\n",
    "        pca = PCA(n_components=2)\n",
    "        pca=pca.fit(df)\n",
    "        components=pca.components_.T\n",
    "        scores = pca.fit_transform(df)\n",
    "        save_pca_graph(scores,components,tumo_type,name,name_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0db75",
   "metadata": {},
   "source": [
    "## 2) PLS (Partial Least Square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_peakTable = '/'.join(os.getcwd().split('/')[:-1]) + '/data/peakTable/original_peak_table/peakTable_HILIC_POS.csv'\n",
    "peakTable = pd.read_csv(path_peakTable, sep=',', decimal='.', na_values='NA')\n",
    "first_cols = peakTable.iloc[:, ['variable' not in col for col in peakTable.columns]]\n",
    "first_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c697a3",
   "metadata": {},
   "source": [
    "###  adding patient data to normlised peaklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/'.join(os.getcwd().split('/')[:-1]) + '/data/peakTable/imputed_peak_tables/X_python_MICE_BayesianRidge.csv')\n",
    "data_nm = nsf.normPeakTable(data, 'mean_normalisation', based='samples')\n",
    "data_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([first_cols, data_nm], axis=1)\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35721745-942a-454d-9b1f-034b36675c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Binary Y vector for stratifiying the samples\n",
    "outcomes = full_data['TypTumo']                                  # Column that corresponds to Y class (should be 2 groups)\n",
    "Y = [1 if outcome == 'HCC' or outcome == 'HCC_Wide' else 0 for outcome in outcomes]       # Change Y into binary (GC = 1, HE = 0)  \n",
    "Y = np.array(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe3797-a8bd-4dd0-85b7-1bc9e6cab711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split full_data and Y into train and test (with stratification)\n",
    "dataTrain, dataTest, Ytrain, Ytest = train_test_split(full_data, Y, test_size=0.25, stratify=Y,random_state=10)\n",
    "\n",
    "print(\"DataTrain = {} samples with {} positive cases ({}%).\".format(len(Ytrain),sum(Ytrain),round(sum(Ytrain)/len(Ytrain),2)))\n",
    "print(\"DataTest = {} samples with {} positive cases ({}%).\".format(len(Ytest),sum(Ytest),round(sum(Ytest)/len(Ytest),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4867a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and scale the metabolite data from the dataTablepeaklist = peakTable.columns[5:]                          # Set peaklist to the metabolite names in the peakTableClean\n",
    "XT = dataTrain[peaklist]                                    # Extract X matrix from DataTrain using peaklist\n",
    "XTlog = np.log(XT)                                          # Log scale (base-10)\n",
    "XTscale = cb.utils.scale(XT, method='auto')              # methods include auto, pareto, vast, and level\n",
    "XTknn = cb.utils.knnimpute(XTscale, k=3)                    # missing value imputation (knn - 3 nearest neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XTknn.shape)\n",
    "print(Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3deaab-6113-4c61-9f9a-94cd751679af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalise cross_val kfold (stratified) \n",
    "cv = cb.cross_val.kfold(model=cb.model.PLS_SIMPLS,                   # model; we are using the PLS_SIMPLS model\n",
    "                        X=XTknn,                                 \n",
    "                        Y=Ytrain,                               \n",
    "                        param_dict={'n_components': [1,2,3,4,5,6]},  # The numbers of latent variables to search                \n",
    "                        folds=5,                                     # folds; for the number of splits (k-fold)\n",
    "                        bootnum=100)                                 # num bootstraps for the Confidence Intervals\n",
    "\n",
    "# run the cross validation\n",
    "cv.run()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39795887-90ad-4732-81a7-a41ab05cfac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcba12-4e2a-4d30-96ec-81b72692a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no idea how to interpret these plots :/ sorrrrrryyyyyyyy jpp c'est flou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb889a-a4e9-4737-b157-b1c5f368d8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defab3d-c66f-45e6-a632-97508d6737e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9d5aa-bed4-47fc-9beb-0a49667524d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a357946-2c43-4bce-9939-3d4d7c8e7be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
